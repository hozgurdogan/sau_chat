{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac33a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c31e40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 11 key-value pairs and 448 tensors from C:\\Users\\mehme\\Desktop\\gguf\\llama3-8b-lora.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = adapter\n",
      "llama_model_loader: - kv   2:                               adapter.type str              = lora\n",
      "llama_model_loader: - kv   3:                               general.name str              = Llama Rag\n",
      "llama_model_loader: - kv   4:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   5:                  general.base_model.0.name str              = Llama 3 Trendyol LLM 8b Chat v2.0\n",
      "llama_model_loader: - kv   6:               general.base_model.0.version str              = v2.0\n",
      "llama_model_loader: - kv   7:          general.base_model.0.organization str              = Trendyol\n",
      "llama_model_loader: - kv   8:              general.base_model.0.repo_url str              = https://huggingface.co/Trendyol/Llama...\n",
      "llama_model_loader: - kv   9:                         adapter.lora.alpha f32              = 16.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  10:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  448 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = all F32 (guessed)\n",
      "print_info: file size   = 640.00 MiB (32.00 BPW) \n",
      "llama_model_load: error loading model: error loading model hyperparameters: key not found in model: llama.context_length\n",
      "llama_model_load_from_file_impl: failed to load model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to load model from file: C:\\Users\\mehme\\Desktop\\gguf\\llama3-8b-lora.gguf. Ensure the file is valid and compatible.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 10\u001b[0m \tmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\programs\\code\\anaconda3\\envs\\rag_test\\lib\\site-packages\\llama_cpp\\llama.py:372\u001b[0m, in \u001b[0;36mLlama.__init__\u001b[1;34m(self, model_path, n_gpu_layers, split_mode, main_gpu, tensor_split, rpc_servers, vocab_only, use_mmap, use_mlock, kv_overrides, seed, n_ctx, n_batch, n_ubatch, n_threads, n_threads_batch, rope_scaling_type, pooling_type, rope_freq_base, rope_freq_scale, yarn_ext_factor, yarn_attn_factor, yarn_beta_fast, yarn_beta_slow, yarn_orig_ctx, logits_all, embedding, offload_kqv, flash_attn, no_perf, last_n_tokens_size, lora_base, lora_scale, lora_path, numa, chat_format, chat_handler, draft_model, tokenizer, type_k, type_v, spm_infill, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel path does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[0;32m    371\u001b[0m     contextlib\u001b[38;5;241m.\u001b[39mclosing(\n\u001b[1;32m--> 372\u001b[0m         \u001b[43minternals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    377\u001b[0m     )\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# Override tokenizer\u001b[39;00m\n",
      "File \u001b[1;32md:\\programs\\code\\anaconda3\\envs\\rag_test\\lib\\site-packages\\llama_cpp\\_internals.py:56\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[1;34m(self, path_model, params, verbose)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m vocab \u001b[38;5;241m=\u001b[39m llama_cpp\u001b[38;5;241m.\u001b[39mllama_model_get_vocab(model)\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to load model from file: C:\\Users\\mehme\\Desktop\\gguf\\llama3-8b-lora.gguf",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \tmodel \u001b[38;5;241m=\u001b[39m Llama(model_path\u001b[38;5;241m=\u001b[39mmodel_path, n_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8192\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 12\u001b[0m \t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to load model from file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Ensure the file is valid and compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to load model from file: C:\\Users\\mehme\\Desktop\\gguf\\llama3-8b-lora.gguf. Ensure the file is valid and compatible."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_path = \"C:\\\\Users\\\\mehme\\\\Desktop\\\\gguf\\\\llama3-8b-lora.gguf\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(model_path):\n",
    "\traise FileNotFoundError(f\"Model file not found at: {model_path}\")\n",
    "\n",
    "try:\n",
    "\tmodel = Llama(model_path=model_path, n_ctx=8192)\n",
    "except ValueError as e:\n",
    "\traise ValueError(f\"Failed to load model from file: {model_path}. Ensure the file is valid and compatible.\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76c53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
